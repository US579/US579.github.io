<!DOCTYPE html>
<html lang="en">
  <head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    <title>US579-blog | HMM </title>
    <meta name="description" content>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    

    <!-- fonts -->
    <link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
    <link href="//fonts.googleapis.com/css?family=Ubuntu:300,400,500,600,700" rel="stylesheet">

    <!-- stylesheets -->
    <link rel="stylesheet" href="/style/doc.css">

    <!-- favicon -->
    <link rel="icon" href="/images/favicon.ico">

    

  </head>
  <body>

   <script>window.__INITIAL_STATE__ = {"page":{"title":"HMM","path":"2019/12/08/HMM/hmm/"},"data":{"navigation":{"logo":{"text":"US579","type":"link","path":"index.html"},"main":[{"text":"ROUTINE","type":"label"},{"text":"MACHINE LEARNING","type":"label"},{"text":"Optimal Binning Algorithm","type":"link","path":"public/2019/12/09/Binning/binning/index.html"},{"text":"HMM","type":"link","path":"2019/12/08/HMM/hmm/index.html"},{"text":"ABOUT ME","type":"label"},{"text":"about","type":"link","path":"about/me.html"}]}},"config":{"timezone":"UTC","root":"/","time_format":"HH:mm:ss","theme":"../node_modules/hexo-theme-doc","theme_config":{"swagger_ui":{"version":2,"permalinks":true,"api_explorer":true,"download":"Download specification","show_extensions":false,"deep_linking":true,"display_operation_id":false,"doc_expansion":"none"},"search":{"skip":false,"background":false,"route":"/lunr.json"},"favicon":"images/favicon.ico"}}}</script>

    <div id="react-navigation-root"><div class="doc-navigation" data-reactroot><nav class="doc-navbar"><a href="/index.html" class="doc-navbar__logo"><img src="/images/logo.png" class="doc-navbar__logo__img"><span class="doc-navbar__logo__text">US579</span></a><i class="dc-icon dc-icon--close dc-icon--interactive doc-sidebar-close doc-navbar__sidebar-close doc-navbar__sidebar-close--desktop"></i><i class="dc-icon dc-icon--menu dc-icon--interactive doc-sidebar-toggle doc-navbar__sidebar-toggle"></i></nav><nav class="doc-sidebar"><div class="doc-sidebar__vertical-menu"><i class="dc-icon dc-icon--menu dc-icon--interactive doc-sidebar-toggle doc-sidebar-toggle--primary doc-sidebar__vertical-menu__item"></i><i class="dc-icon dc-icon--search dc-icon--interactive doc-sidebar__vertical-menu__item doc-sidebar__vertical-menu__item--primary"></i></div><div class="doc-sidebar-content"><div class="doc-sidebar__search-form"></div><ul class="doc-sidebar-list"></ul></div></nav></div></div>
    <div class="doc-content">
  <div class="dc-page">
    <div class="dc-card">
      <div id="react-search-results-root"></div>
      <div id="page-content" class="doc-formatting">
        <h1 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model  "></a>Hidden Markov Model  <img src="https://img.shields.io/static/v1.svg?label=US579&amp;message=HMM&amp;color=&lt;BLUE" alt="US579"></h1><h3 id="repo"><a href="#repo" class="headerlink" title="repo"></a>repo</h3><p><a href="https://github.com/US579/DataMining-Project" target="_blank" rel="noopener">https://github.com/US579/DataMining-Project</a></p>
<h2 id="How-to-run"><a href="#How-to-run" class="headerlink" title="How to run"></a>How to run</h2><p>Dataset in this case :<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">State_File =&apos;./toy_example/State_File&apos;</span><br><span class="line">Symbol_File=&apos;./toy_example/Symbol_File&apos;</span><br><span class="line">Query_File =&apos;./toy_example/Query_File&apos;</span><br></pre></td></tr></table></figure></p>
<p>simply run:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 submission.py</span><br></pre></td></tr></table></figure></p>
<h2 id="HMM-and-Viterbi-Algorithem-description"><a href="#HMM-and-Viterbi-Algorithem-description" class="headerlink" title="HMM and Viterbi Algorithem description"></a>HMM and Viterbi Algorithem description</h2><p>In this case below, the output of the implicit sequence is</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 0, 0, 1, 2, 4, -9.843403381747937]</span><br></pre></td></tr></table></figure>
<p>with log probility <code>-9.843403381747937</code> which is largest probility</p>
<h5 id="Parameter-breakdown"><a href="#Parameter-breakdown" class="headerlink" title="Parameter breakdown"></a>Parameter breakdown</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0:S1       1:S2      2:S3     3:BEGIN      4:END</span><br></pre></td></tr></table></figure>
<p><img src="/images/hmm.png" alt title="hmm"></p>
<h3 id="1-Initial-Probilities"><a href="#1-Initial-Probilities" class="headerlink" title="1.Initial Probilities"></a>1.Initial Probilities</h3><p>The blue line represent the  initial probability (Pi) which can be deemed as equivalent to transition probabilities from the BEGIN state to all the implicit state</p>
<p>So, we caculate as<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> states[:<span class="number">-2</span>]:</span><br><span class="line">    transition_probability[len(states)<span class="number">-2</span>][states.index(s)])</span><br></pre></td></tr></table></figure></p>
<h3 id="2-Emission-Probilities"><a href="#2-Emission-Probilities" class="headerlink" title="2.Emission Probilities"></a>2.Emission Probilities</h3><p>The red line represent its emission probability from state after smoothing is </p>
<div align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=B[i,j]=\frac{n(i,j)&plus;1}{n(i)&plus;M&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?B[i,j]=\frac{n(i,j)&plus;1}{n(i)&plus;M&plus;1}" title="B[i,j]=\frac{n(i,j)+1}{n(i)+M+1}"></a></div>

<p>If the symbol is an unknown symbol, its emission probability from state after smoothing is</p>
<div align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=B[i,j]=\frac{1}{n(i)&plus;M&plus;1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?B[i,j]=\frac{1}{n(i)&plus;M&plus;1}" title="B[i,j]=\frac{1}{n(i)+M+1}"></a></div>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(obs)):</span><br><span class="line">    <span class="keyword">for</span> cur <span class="keyword">in</span> range(len(states[:<span class="number">-2</span>])):</span><br><span class="line">                <span class="comment">#if there is no emission from states `cur` to observation `sym.index(obs[i])`(this is the index in the symbol list),we us add one smoothing (in case it is 0)</span></span><br><span class="line">                <span class="keyword">if</span> str(cur)+<span class="string">'-'</span>+str(sym.index(obs[i])) <span class="keyword">not</span> <span class="keyword">in</span> emission_probability:</span><br><span class="line">                    emission_rate = <span class="number">1.0</span> / (dic_distance[cur] + n2 +<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment">#otherwise i will use the formula below</span></span><br><span class="line">                    emission_rate = emission_probability[str(cur)+<span class="string">'-'</span>+str(sym.index(obs[i]))]</span><br></pre></td></tr></table></figure>
<h3 id="2-Transition-Probilities"><a href="#2-Transition-Probilities" class="headerlink" title="2.Transition Probilities"></a>2.Transition Probilities</h3><p>The black line represent the transition probilities transfer the states from on to another </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n1):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n1):</span><br><span class="line">        transition_probability[i][j] = (float(distance[i][j])+<span class="number">1</span>) / (sum(distance[i])+n1<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<p>the number of state_i transfer to state_j divide by the total number of transfering state_j to any states , i also use add-1 smoothing here</p>
<h3 id="3-Viterbi-Algorithm"><a href="#3-Viterbi-Algorithm" class="headerlink" title="3.Viterbi Algorithm"></a>3.Viterbi Algorithm</h3><p>for HMM, the most useful function is to find the most likely implicit sequence according to its observation,In general, the HMM problem can be described by the following five elements:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">observations ：we observed phenomenon sequence</span><br><span class="line">states ：all the possible implicit states</span><br><span class="line">start_probability ：the initial probilities of each implicit states</span><br><span class="line">transition_probability ：the probility of transfering from one implicit states to another</span><br><span class="line">emission_probability ：the probility of some implicit states emit some observed phenomenon</span><br></pre></td></tr></table></figure></p>
<p>If you use the brute-force method to exhaust all possible state sequences and compare their probability values, the time complexity is O(n^m), obviously , this is unacceptable when we want to find a long sequnce with large dataset, however, we can decrease its time complexity by using Viterbi Algorithem, </p>
<p>we can consider this probelm as dynamic programming , the last_state is the probability of each implicit state corresponding to the previous observed phenomenon, and curr_pro is the probability of each implicit state corresponding to the current observed phenomenon. Solving cur_pro actually depends only on last_state, this is core thinking of Vitberi Algorithem.</p>
<h4 id="MAIN-ALGORITHEM"><a href="#MAIN-ALGORITHEM" class="headerlink" title="MAIN ALGORITHEM"></a>MAIN ALGORITHEM</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Viterbi_Algorithm</span><span class="params">(states,obs,emission_probability,transition_probability)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        caculate maximum probility of the path , and return as a dict,</span></span><br><span class="line"><span class="string">        :argument</span></span><br><span class="line"><span class="string">            :type dict</span></span><br><span class="line"><span class="string">            states : the states </span></span><br><span class="line"><span class="string">            emission_probability : the two-dimension array cotains the emission probility(I use dict here) </span></span><br><span class="line"><span class="string">            transition_probability : the two-dimension array cotains the transition probility</span></span><br><span class="line"><span class="string">            obs : observation sequence</span></span><br><span class="line"><span class="string">        :return</span></span><br><span class="line"><span class="string">            A dic</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">            <span class="comment"># caculate the initial state probility also count the first emission probility from observation</span></span><br><span class="line">            curr_pro[s] = math.log(transition_probability[len(states)<span class="number">-2</span>][states.index(s)])+\</span><br><span class="line">                          math.log(emission_probability[states.index(s))],[sym.index(obs[<span class="number">0</span>]))])</span><br><span class="line">        <span class="comment">#caculate the rest obervation sequence</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(obs)):</span><br><span class="line">            last_pro = curr_pro</span><br><span class="line">            curr_pro = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> cur <span class="keyword">in</span> range(len(states)):</span><br><span class="line">                    (max_pr,last_state) = max([(last_pro[k]+math.log(transition_probability[states.index(k)][cur])+</span><br><span class="line">                                                math.log(emission_rate), k) <span class="keyword">for</span> k <span class="keyword">in</span> states])</span><br><span class="line">                curr_pro[states[cur]] = max_pr</span><br><span class="line">                path[states[cur]].append(last_state)</span><br><span class="line">        <span class="keyword">return</span> path</span><br></pre></td></tr></table></figure>
<h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><p>WANZE LIU</p>

        <div id="react-support-footer-root"></div>
      </div>
    </div>
  </div>
</div>

    


    

    <!-- js vendors -->
    <script src="//code.jquery.com/jquery-3.2.1.min.js" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/lunr.js/2.1.0/lunr.min.js"></script>

    <!-- js source  -->
    <script src="/script/doc.js"></script>

    

  </body>
</html>
